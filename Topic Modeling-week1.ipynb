{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 550332 characters\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "poeUrl = \"http://www.gutenberg.org/files/2147/2147-0.txt\"\n",
    "poeString = urllib.request.urlopen(poeUrl).read().decode(\"GBK\",'ignore').strip()\n",
    "print(\"This string has\", len(poeString), \"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 550,332 characters\n"
     ]
    }
   ],
   "source": [
    "poeStringLen = len(poeString)\n",
    "poeStringLenFormatted = \"{:,}\".format(poeStringLen) # format mini-language\n",
    "print(\"This string has\", poeStringLenFormatted, \"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First character: 锘\n",
      "Last character .\n",
      "First 25 characters: 锘縋roject Gutenberg鈥檚 The \n",
      "Last 25 characters: to hear about new eBooks.\n",
      "Characters 8 to 25:  Gutenberg鈥檚 The Works\n"
     ]
    }
   ],
   "source": [
    "print(\"First character:\", poeString[0])\n",
    "print(\"Last character\", poeString[-1])\n",
    "print(\"First 25 characters:\", poeString[:25])\n",
    "print(\"Last 25 characters:\", poeString[-25:])\n",
    "print(\"Characters 8 to 25:\", poeString[8:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of 'corpse': 65\n"
     ]
    }
   ],
   "source": [
    "print(\"Occurrences of 'corpse':\", poeString.count(\"corpse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of 'corpse': 65\n",
      "Occurrences of 'corps': 65\n",
      "Occurrences of 'Corpse': 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Occurrences of 'corpse':\", poeString.count(\"corpse\"))\n",
    "print(\"Occurrences of 'corps':\", poeString.count(\"corpse\"))\n",
    "print(\"Occurrences of 'Corpse':\", poeString.count(\"Corpse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of 'CORPSE': 65\n"
     ]
    }
   ],
   "source": [
    "print(\"Occurrences of 'CORPSE':\", poeString.upper().count(\"CORPSE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and (horrible to relate!) the corpse of the daughter, head\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstCorpus = poeString.find(\"corpse\") # the index position of the first occurrence of \"corpse\"\n",
    "context = 30 # number of characters to show on either side of the index position\n",
    "print(poeString[firstCorpus-context : firstCorpus+context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GOLD-BUG\r\n",
      "\r\n",
      "          What ho! what ho! this f […]  it; perhaps it\r\n",
      "required a dozen--who shall tell?鈥\n"
     ]
    }
   ],
   "source": [
    "start = poeString.find(\"THE GOLD-BUG\")\n",
    "end = poeString.find(\"FOUR BEASTS IN ONE\")\n",
    "goldBugString = poeString[start:end].strip()\n",
    "\n",
    "# show start and end of goldBugString\n",
    "print(goldBugString[:50], \"[…] \", goldBugString[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = \"data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/goldBug.txt\", \"w\") as f:\n",
    "    f.write(goldBugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/goldBug.txt\", \"r\") as f:\n",
    "    goldBugString2 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GOLD-BUG\n",
      "\n",
      "\n",
      "\n",
      "          What ho! what ho! this f […]  it; perhaps it\n",
      "\n",
      "required a dozen--who shall tell?鈥\n"
     ]
    }
   ],
   "source": [
    "print(goldBugString2[:50], \"[…] \", goldBugString2[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldBugString == goldBugString2 # are these two strings the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\goldBug.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "textFiles = glob.glob(\"data/*txt\")\n",
    "textFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(textFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\goldBug.txt has 77917 characters\n",
      "total characters:  77917\n"
     ]
    }
   ],
   "source": [
    "totalCharacters = 0\n",
    "for textFile in textFiles:\n",
    "    f = open(textFile, \"r\")\n",
    "    textString = f.read()\n",
    "    f.close()\n",
    "    chars = len(textString)\n",
    "    print(textFile, \"has\", chars, \"characters\")\n",
    "    totalCharacters += chars\n",
    "print(\"total characters: \", totalCharacters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download() # download NLTK data (we should only need to run this cell once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/goldBug.txt\", \"r\") as f:\n",
    "    goldBugString = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "goldBugTokens = nltk.word_tokenize(goldBugString)\n",
    "goldBugTokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  technique 1 where we create a new list\n",
    "loopList = []\n",
    "for word in goldBugTokens[:10]:\n",
    "    loopList.append(word)\n",
    "print(loopList, \"(for loop technique)\")\n",
    "    \n",
    "    \n",
    "# technique 2 with list comprehension\n",
    "print([word for word in goldBugTokens[:10]], \"(list comprehension technique)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word for word in goldBugTokens[:10] if word[0].isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugRealWordTokensSample = [word for word in goldBugTokens[:10] if word[0].isalpha()]\n",
    "goldBugRealWordFrequenciesSample = nltk.FreqDist(goldBugRealWordTokensSample)\n",
    "goldBugRealWordFrequenciesSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugRealWordFrequenciesSample.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugTokensLowercase = nltk.word_tokenize(goldBugString.lower()) # use lower() to convert entire string to lowercase\n",
    "goldBugRealWordTokensLowercaseSample = [word for word in goldBugTokensLowercase[:10] if word[0].isalpha()]\n",
    "goldBugRealWordFrequenciesSample = nltk.FreqDist(goldBugRealWordTokensLowercaseSample)\n",
    "goldBugRealWordFrequenciesSample.tabulate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugTokensLowercase = nltk.word_tokenize(goldBugString.lower())\n",
    "goldBugRealWordTokensLowercase = [word for word in goldBugTokensLowercase if word[0].isalpha()]\n",
    "goldBugRealWordFrequencies = nltk.FreqDist(goldBugRealWordTokensLowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of types: \", len(goldBugRealWordFrequencies))\n",
    "print(\"number of tokens: \", len(goldBugRealWordTokensLowercase))\n",
    "print(\"type/token ratio: \", len(goldBugRealWordFrequencies)/len(goldBugRealWordTokensLowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugRealWordFrequencies.tabulate(20) # show a sample of the top frequency terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(sorted(stopwords)) # sort them alphabetically before printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample words: \", goldBugRealWordTokensLowercaseSample)\n",
    "print(\"sample words not in stopwords list: \", [word for word in goldBugRealWordTokensLowercaseSample if not word in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugRealContentWordTokensLowercase = [word for word in goldBugTokensLowercase \\\n",
    "        if word[0].isalpha() and word not in stopwords]\n",
    "goldBugRealContentWordFrequencies = nltk.FreqDist(goldBugRealContentWordTokensLowercase)\n",
    "goldBugRealContentWordFrequencies.tabulate(20) # show a sample of the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldBugText = nltk.Text(goldBugTokens)\n",
    "goldBugText.concordance(\"de\", lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "sonnetsUrl = \"http://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
    "sonnetsString = urllib.request.urlopen(sonnetsUrl).read().decode()\n",
    "import re, os\n",
    "pythonfilteredSonnetsStart = sonnetsString.find(\"  I\\r\\n\") # title of first sonnet\n",
    "filteredSonnetsEnd = sonnetsString.find(\"End of Project Gutenberg's\") # end of sonnets\n",
    "filteredSonnetsString = sonnetsString[filteredSonnetsStart:filteredSonnetsEnd].rstrip()\n",
    "sonnetsList = re.split(\"  [A-Z]+\\r\\n\\r\\n\", filteredSonnetsString)\n",
    "sonnetsPath = 'sonnets' # this subdirectory will be relative to the current notebook\n",
    "if not os.path.exists(sonnetsPath):\n",
    "    os.makedirs(sonnetsPath)\n",
    "for index, sonnet in enumerate(sonnetsList): # loop through our list as enumeration to get index\n",
    "    if len(sonnet.strip()) > 0: # make sure we have text, not empty after stripping out whitespace\n",
    "        filename = str(index).zfill(3)+\".txt\" # create filename from index\n",
    "        pathname = os.path.join(sonnetsPath, filename) # directory name and filenamee\n",
    "        f = open(pathname, \"w\")\n",
    "        f.write(sonnet.rstrip()) # write out our sonnet into the file\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "sonnetsUrl = \"http://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
    "sonnetsString = urllib.request.urlopen(sonnetsUrl).read().decode()\n",
    "len(sonnetsString) # let's make sure we have a lot of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredSonnetsStart = sonnetsString.find(\"  I\\r\\n\") # title of first sonnet\n",
    "filteredSonnetsEnd = sonnetsString.find(\"End of Project Gutenberg's\") # end of sonnets\n",
    "filteredSonnetsString = sonnetsString[filteredSonnetsStart:filteredSonnetsEnd].rstrip() # strip spaces from the end (right)\n",
    "print(filteredSonnetsString[:75] + ' … ' + filteredSonnetsString[-75:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sonnetsList = re.split(\"  [A-Z]+\\r\\n\\r\\n\", filteredSonnetsString)\n",
    "print(sonnetsList[1]) # sonnetList[0] is empty (the text before the first roman numeral), let's look at sonnet I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sonnetsPath = 'sonnets' # this subdirectory will be relative to the current notebook\n",
    "if not os.path.exists(sonnetsPath):\n",
    "    os.makedirs(sonnetsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(1).zfill(3)) # convert the number 1 to a string and prepend zeros as needed to get three characters\n",
    "print(str(150).zfill(3)) # convert the number 150 to a string and prepend zeros as needed to get three characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sonnet in enumerate(sonnetsList): # loop through our list as enumeration to get index\n",
    "    if len(sonnet.strip()) > 0: # make sure we have text, not empty after stripping out whitespace\n",
    "        filename = str(index).zfill(3)+\".txt\" # create filename from index\n",
    "        pathname = os.path.join(sonnetsPath, filename) # directory name and filenamee\n",
    "        f = open(pathname, \"w\")\n",
    "        f.write(sonnet.rstrip()) # write out our sonnet into the file (removing whitespace at the end/right)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "sonnetsCorpus = PlaintextCorpusReader(sonnetsPath, '.*txt')\n",
    "len(sonnetsCorpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_summary(corpus):\n",
    "    print(\"this corpus has\")\n",
    "    print(\"  \", '{:,}'.format(len(sonnetsCorpus.fileids())), \"files\")\n",
    "    tokens = corpus.words() # this includes *all* words in all files\n",
    "    print(\"  \", '{:,}'.format(len(tokens)), \"tokens\")\n",
    "    words = [word for word in tokens if word[0].isalpha()]\n",
    "    print(\"  \", '{:,}'.format(len(tokens)), \"words\")\n",
    "    print(\"  \", '{:,}'.format(len(set(tokens))), \"unique word types\")\n",
    "\n",
    "corpus_summary(sonnetsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "sonnetsCorpus = PlaintextCorpusReader(\"sonnets\", \".*\\.txt\")\n",
    "print(len(sonnetsCorpus.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_lists_of_words(corpus, **kwargs): # the ** in front of kwargs does the magic of keyword arguments\n",
    "    documents = [] # list of documents where each document is a list of words\n",
    "    for fileid in corpus.fileids(): # go trough each file in our corpus\n",
    "        \n",
    "        # keep only words and convert them to lowercase\n",
    "        words = [token.lower() for token in corpus.words(fileid) if token[0].isalpha()]\n",
    "        \n",
    "        # look for \"minLength\" in our keyword arguments and if it's defined, filter our list\n",
    "        if \"minLen\" in kwargs and kwargs[\"minLen\"]: \n",
    "            words = [word for word in words if len(word) >= kwargs[\"minLen\"]]\n",
    "        \n",
    "        # look for \"stopwords\" in our keyword arguments and if any are defined, filter our list\n",
    "        if \"stopwords\" in kwargs and kwargs[\"stopwords\"]: \n",
    "            words = [word for word in words if word not in kwargs[\"stopwords\"]]\n",
    "\n",
    "        # look for \"pos\" in our keyword arguments and if any are defined, filter our list\n",
    "        if \"pos\" in kwargs and kwargs[\"pos\"]: \n",
    "            tagged = nltk.pos_tag(words)\n",
    "            words = [word for word, pos in tagged if pos in kwargs[\"pos\"]]\n",
    "        \n",
    "        documents.append(words) # add our list of words\n",
    "    \n",
    "    return documents # return our list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnetsStopwords = nltk.corpus.stopwords.words('english') # load the default stopword list\n",
    "sonnetsStopwords += [\"thee\", \"thou\", \"thy\"] # append a few more obvious words\n",
    "sonnetsWords = get_lists_of_words(sonnetsCorpus, stopwords=sonnetsStopwords, minLen=3)\n",
    "\n",
    "# have a peek:\n",
    "for i in range(0,2): # first two documents\n",
    "    print(\"document\", str(i), sonnetsWords[i][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lda_from_lists_of_words(lists_of_words, **kwargs):\n",
    "    dictionary = corpora.Dictionary(lists_of_words) # this dictionary maps terms to integers\n",
    "    corpus = [dictionary.doc2bow(text) for text in lists_of_words] # create a bag of words from each document\n",
    "    tfidf = models.TfidfModel(corpus) # this models the significance of words by document\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    kwargs[\"id2word\"] = dictionary # set the dictionary\n",
    "    return models.LdaModel(corpus_tfidf, **kwargs) # do the LDA topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnetsLda = get_lda_from_lists_of_words(sonnetsWords, num_topics=10, passes=20) # small corpus, so more passes\n",
    "print(sonnetsLda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_terms(lda, num_terms=10):\n",
    "    for i in range(0, lda.num_topics):\n",
    "        terms = [term for val, term in lda.show_topic(i, num_terms)]\n",
    "        print(\"Top 10 terms for topic #\", str(i), \": \", \", \".join('%s' % id for id in terms))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_terms(sonnetsLda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "G = nx.Graph()\n",
    "G.add_edge(\"A\", \"X\") # student A went to school X\n",
    "G.add_edge(\"A\", \"Y\") # student A went to school Y\n",
    "G.add_edge(\"B\", \"X\") # student B went to school X\n",
    "G.add_edge(\"C\", \"Y\") # student C went to school X\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_labels(G, pos, font_color='r') # font colour is \"r\" for red\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.1) # set the line alpha transparency to .1\n",
    "plt.axis('off') # don't show the axes for this plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graph_terms_to_topics(lda, num_terms=10):\n",
    "    \n",
    "    # create a new graph and size it\n",
    "    G = nx.Graph()\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    # generate the edges\n",
    "    for i in range(0, lda.num_topics):\n",
    "        topicLabel = \"topic \"+str(i)\n",
    "        terms = [term for term, val in lda.show_topic(i, num_terms)]\n",
    "        for term in terms:\n",
    "            G.add_edge(topicLabel, term)\n",
    "    \n",
    "    pos = nx.spring_layout(G) # positions for all nodes\n",
    "    \n",
    "    # we'll plot topic labels and terms labels separately to have different colours\n",
    "    g = G.subgraph([topic for topic, _ in pos.items() if \"topic \" in topic])\n",
    "    nx.draw_networkx_labels(g, pos,  font_color='r')\n",
    "    g = G.subgraph([term for term, _ in pos.items() if \"topic \" not in term])\n",
    "    nx.draw_networkx_labels(g, pos)\n",
    "    \n",
    "    # plot edges\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=G.edges(), alpha=0.1)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "graph_terms_to_topics(sonnetsLda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "sentenceLengths = {}\n",
    "for fileid in nltk.corpus.gutenberg.fileids():\n",
    "    sentenceLengths[fileid] = len(nltk.corpus.gutenberg.sents(fileid)) / len(nltk.corpus.gutenberg.words(fileid))\n",
    "\n",
    "nltk.FreqDist(sentenceLengths).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "documents = [\n",
    "    \"I love jazz music and I love to read good books\",\n",
    "    \"What good books have you read recently\",\n",
    "    \"It's music to my ears when you say I love you\"\n",
    "]\n",
    "\n",
    "allWords = set([word.lower() for word in nltk.word_tokenize(\" \".join(documents)) if any([c for c in word if c.isalpha()])])\n",
    "allRawFreqs = [nltk.FreqDist(nltk.word_tokenize(document.lower())) for document in documents]\n",
    "pd.DataFrame(allRawFreqs).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "simpledtm = TfidfVectorizer().fit_transform(documents) # takes a list of strings (and tokenizes them for us)\n",
    "pd.DataFrame(simpledtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.gutenberg.fileids() # we'll keep track of filenames (for labels)\n",
    "texts = [nltk.corpus.gutenberg.raw(fileid) for fileid in names] # a list of strings, one per document\n",
    "documentTermMatrix = TfidfVectorizer().fit_transform(texts)\n",
    "documentTermMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "distances = 1 - cosine_similarity(documentTermMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "# reduce our n-dimensional distances matrix to a two dimensional matrix (for x and y coordinates)\n",
    "mds = MDS(dissimilarity=\"precomputed\", random_state=1)\n",
    "positions = mds.fit_transform(distances)\n",
    "\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xvalues = positions[:, 0] # the left colunn (x axis) for all rows\n",
    "yvalues = positions[: ,1] # the right column (y axis) for all rows\n",
    "\n",
    "plt.figure(figsize=(10,10)) # make the graph easier to see\n",
    "for x, y, name in zip(xvalues, yvalues, names):\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x, y, name.replace(\".txt\", \"\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "linkage_matrix = ward(distances)\n",
    "dendrogram(linkage_matrix, labels=names, orientation=\"right\");\n",
    "plt.show()  # fixes margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
